% $Id: w05_spreading.tex gnawux $
%  vim: set ft=tex:
\chapter{移动个人网络中的分布式系统信息存储}\label{ch:data}

\section{引言}

分布式终端系统中的高级节点的关键任务之一是维护系统的状态，即分布式地存储系统中的可用资源信息与系统中的业务应用的状态信息，保障这些信息在移动过程中的可靠性和节点大量变动的情况下数据存取的高效性，同时，数据维持工作也应该较低地消耗节点的计算、存储、通信带宽和能量资源，尽量少地影响系统中运行的其它应用。

作为一个分布式系统，分布式终端系统可以采用有中心或者无中心两大类方式来存储这些系统中的资源和状态信息，前者的优点在于数据的组织管理开销比较低，只有少数中心节点需要维护管理数据、处理数据一致性问题，但是，由于用户周边设备的稳定性较低、能力有限，这种依赖于少数节点的工作方式的可靠性和可扩展性都比较有限；另一方面，从~20~世纪末开始，对等网络~(P2P)~技术逐渐在互联网上流行并进一步拓展到包括移动自组织网络在内的各种计算机网络之上，通过节点间的协同来存储数据，不再受限于单一节点的能力，完全避免了中心节点失效导致整个系统无法工作的问题，极大地提高了系统的可用性和可扩展性。

本章试图通过对移动个人网络中的系统相关信息与其工作方式的分析，给出一种与其他相关工作相比适用于此种网络的系统信息的分布式存储方式。

\section{分布式系统的系统资源数据存储}

本章所考虑的分布式资源存储并非一个分布式文件系统，因为这里要存储的数据规格一致、具有很强的结构性，即系统可用资源描述与应用状态，而并不存在非格式化、二进制数据，不存在目录、管道、链接等特殊类型，因此，其存储的数据实际是一些目录数据，除了数据库之外，很多其它互联网上的协议~(如表~\ref{tab:dirproto}~所列)~也都支持这类操作，其共性是
\begin{itemize}
	\item 以~``请求/响应''~方式工作，发出查询请求、得到查询结果；
	\item 查询请求可以通过某些限制条件获得符合条件的若干记录，或得到请求无法满足的响应结果；
	\item 响应结果使用协议规定的标准格式，具有明确语义；
	\item 部分协议支持~``注册/声明''~操作，用以发布信息。
\end{itemize}
其工作方式如图~\ref{fig:dirproto}~所示。

\begin{table}[tb]
	\centering
	\begin{tabular}{lp{.6\textwidth}}
		\hline
		协议名称 & 描述 \\
		\hline
		SLP		& Service Location Protocol (RFC2608), 服务定位协议 \\
		LDAP	& Lightweight Directory Location Protocol, 轻量级目录访问协议 \\
		NIS     & Network Information Service, 网络信息服务~(黄页服务) \\
		DNS		& Domain Name Service, 域名服务\\
		Jini	& Jini\texttrademark~的核心之一是一个查询服务\\
		SSDP	& Simple Service Discovery Protocol, 简单服务发现协议，UPnP~中的服务发现协议\\
		\hline
	\end{tabular}
	\caption{具有类似目录查询工作方式的网络协议举例}
	\label{tab:dirproto}
\end{table}

\begin{figure}[tb]
	\begin{center}
		\includegraphics[width=.5\textwidth]{luproto}
	\end{center}
	\caption{目录查询工作方式示意}
	\label{fig:dirproto}
\end{figure}

作为网络通信协议，表~\ref{tab:dirproto}~所列的协议一般都只规范网络上传播的~``查询/注册''~消息，而不规范数据的存储方式，在通常的实现中，这些目录数据会存储于服务器本地的数据结构之中，但通过构建一个~P2P~的层叠网络，将数据存储于这个网络之内，也可以完成类似的工作。

\subsection{传统的有中心存储方式}

对于表~\ref{tab:dirproto}~中所述的协议，它们的工作的基础都需要首先设定或得到一个目录服务器的地址，以~SLPv2 (SLP~版本~2)~为例，服务提供者~(称为服务代理，Service Agent, SA)~或作为服务的使用者的应用~(称为用户代理，User Agent, UA)~可以通过主动和被动两种方法获得提供目录的服务器~(目录代理，Directory Agent, DA)~的地址，而其它协议中，目录服务器的地址则更多依赖于手工指定或是通过自动配置过程~(如~DHCP)~获取。由于这些目录服务的地址一般是一个或几个单播~IP~地址，因此，传统的目录信息的存储方式也天然地是有中心的。

有中心的存储方式在网络中面临着两个主要问题：
\begin{itemize}
	\item 由于网络连接和服务器自身的可靠性是有限的，中心节点或关键链路失效会导致尽管整个系统的大部分元素工作正常，但服务却无法正常进行。
	\item 由于中心节点的存储、计算能力和通信带宽都是有限的，当系统的规模增大的时候，中心节点相关的性能将成为整个系统的瓶颈。
\end{itemize}

当然，有中心架构并非只有一个节点，通过多个中心节点进行互相备份与负载均衡，有中心的存储方式可以在一定程度上缓解这一可靠性和可伸缩性的问题。然而，对于移动个人网络所在的个人域环境，由于个人域设备的能力和可靠性都难以保障，少量的备份不足以让移动个人网络有效避免这一问题，因此，本论文考虑将存储的信息更进一步的分布化，使用无中心架构来存储目录信息。

%As a centralized directory such as that in Service Location Protocol (SLP) or Jini is hard to survive in mobility event of mobile personal networks, decentralized directory and data replication among nodes are adopted in order to improve the robustness. Discrete Hash Table (DHT) based methods, such as chord  and pastry , enable a distributed storage infrastructure for Internet, however, each node in a DHT ring should keep routing information of $O(\log{}N)$ peers and an additional node's joining costs a lookup ($O(\log{}N)$ messages) and updating routing informations in $O(\log{}N)$ nodes . As the nodes often joining a mobile personal network as groups, DHT based infrastructure may not handle it effectively. 

\subsection{非结构化无中心数据存储}

对等~(Peer-to-Peer, P2P)~架构是有别于~``客户机-服务器''~架构的一种分布式系统架构模型，因为不区分客户机与服务器，所以也被称为无中心架构，自从~20~世纪末在文件共享领域被广泛使用之后，逐步引起更多研究者的注意，其突出优点是所有参与者都是客户机也都是服务器，因此不存在任何会影响整个系统工作的关键点，呈现出很强的自组织特性。在诸如~Gnutella~等早期纯粹的~P2P~网络协议中，所有的节点之间是完全没有组织的，节点之间的信息查找使用所谓的~``泛洪''~方式，即从一点起，向所有已知的节点散播查询消息，借到查询消息的节点会把用同样方法得到的信息返回给查找者，当然，一次查找所散播的跳数被限制在一个很有限的值之内，以避免过长的响应时间以及过度占用网络带宽。

由于节点间的松散关系，与下文将要提到的~``结构化''~P2P~相对应，这种基于泛洪式查询的无中心存储架构被称为非结构化的~P2P~架构，这种架构的优点和缺点都十分明显，它的突出优点在于没有任何组织开销，不需要任何节点为整个系统的组织负责，工作方式稳定而可靠，然而，它所带来的最显著的缺点就在于~``不确定性''，即当一次查找没有得到结果的时候，发起查找的客户端无法确定被查找者是根本不存在还是恰恰在自己查找范围之外一点点而已，另一个缺点就是~``泛洪''~式的通信会对网络带宽有较大的影响，且不可管控。

\subsection{结构化无中心数据存储}\label{sss:05:structp2p}

所谓~``结构化''~无中心架构是利用某种映射关系，将系统中存储的元素与存储这些元素的节点相对应起来，从而可以在有限步骤内，查询到一个元素或是确定地知道该元素不存在。一种比较常见的用于结构化~P2P~中查询机制的映射关系是所谓的分布式哈希表~(Distributed Hashing Table, DHT)，即使用一个哈希函数处理存储节点的名称~(地址)~和要存储的数据的名称，将所得到的哈希值排列在一个圆周之上，把哈希值相邻的两个节点~$N_k$~和~$N_{k+1}$~之间的所有要存储的数据都交给节点~$N_{k+1}$~来保存，也就是说，当希望找到哈希值为~$h$~的数据元素，只要找到哈希值刚刚好~``小于''~$h$~的节点，然后通过它确定它的下一个节点的位置，就可以获得目标数据了，记该节点的值为~$\lfloor h\rfloor$，这里，``小于''~是圆周上的小于，如果哈希函数的值域是~$\left[0,H\right)$~的话，$0$~是大于~$H-1$~的。

对于~DHT~方法来说，各个不同的协议或机制的差异就在于给定一个哈希值~$h$，如何找到哈希值为~$\lfloor h\rfloor$~的节点。比较容易理解的方式是每个节点都保存它下一个节点的地址和哈希值，构成一个环形的链，只要一环一环地找，就可以找到需要的节点，这个方法的复杂度显然是~$O(N)$~的，相比于泛洪工作方式的指数蔓延速度，这个确定性的代价实在是太大了。于是，Chord、Pastry~等基于~DHT~的机制都让每个节点多存储一些相关节点的信息，从而加速这个查找过程，以~Chord~为例，通过付出每个节点存储~$O(\log(N))$~个相关节点的信息的代价，使得这个查询过程的复杂度可以减少到~$O(\log(N))$，从而进入一个可以接受的范围。当然，存储相关节点的信息越多，当节点发生变更的时候，在系统中受影响的节点也就越多，管理维护开销也就越大。

DHT~方法中，通过节点之间的交互，逐步地查找目的节点，这一过程和网络层的路由有些类似，只是发生在应用层。除了层面的区别之外，另一个区别就是查找所依据的键值，网络层查找依赖于网络地址，这个地址强烈地反映了节点之间的拓扑位置关系，地址越接近，两个节点之间也越接近，而~DHT~方法的~``应用层路由''~则是完全的数学意义上的接近，和拓扑位置的接近完全无关，因此，这层覆盖在网络层以上的层叠网络~(Overlay)~带来的数据分布在拓扑上的无规律性对于负载均衡等很多考量都是十分有益的，但同时，对于消息传播的效率是也有一定影响。

\section{移动个人网络的分布式数据存储}

具体地考虑移动个人网络中的分布式数据存储，作为~MANET~的一个子集，移动个人网络同样是没有任何基础设施，具有很强的对等与自组织性，这与无中心的应用~Overlay~非常类似，而且，由于移动个人网络中，并不假设设备具有高性能与高可用性，需要避免因部分设备失效而带来的系统服务中断。因此，有必要将移动个人网络中的所有具有一定能力的节点，即高级节点，组织起来，成为一个分布式系统信息存储~Overlay。

对于移动个人网络来说，结构化和非结构化的~P2P~方式均有其各自的不足：非结构化无中心的~P2P~分布式系统通常用于资源共享，没有固定的存储机制，不存在一个确定的~``存储元素-存储节点''~的映射关系，而分布式终端系统则需要一个相对可靠的存储机制，这使得非结构化~P2P~难堪重用。

然而，对于我们的应用，结构化~P2P~也具有它自己的显著不足，回顾~\S~\ref{sse:03:mpnchar}，我们注意到，移动个人网络中的一个重要的移动性特征就是大量节点的同时加入与退出，而正如~\S~\ref{sss:05:structp2p}~所述，结构化的~P2P~方式为了在维护和查询效率中取得折衷，不得不在每个节点上保存~$O(\log(N))$~的其他节点的路由信息，而这些信息会导致每个新加入节点需要一定的时间才能有效地融入系统，达到稳定状态。以~Chord~为例，如果加入节点的速度低于修复路由信息的速度，可以维持基本相同的查询效率，而当大量节点同时加入时，不可避免地会带来更高的维护开销和更长的达到稳定所需的时间。

在这一方面，非结构化由于其无状态性，反而具有较好的性能，不需要过多的维护开销，而且，基于~\S~\ref{sse:03:mpnchar}~的分析，由于移动个人网络中链路数量有限，泛洪搜索方式所造成的冲击是有限的。这样，论文考虑将结构化~P2P~中的确定性部分地引入到非结构化~P2P~的基础架构之中，构成一种~``半确定性''~架构，达到效率和可靠性的折衷。

若移动个人网络中有~$n$~个节点，$N_1, N_2,\cdots,N_n$，它们分布在网络中的~$l$~个异构链路~$L_1, L2,\cdots,L_l$~上，其中有~$s$~个高级节点~($N_{S_1}, N_{S_2},\cdots,N_{S_s}$)~可以用于存储系统信息。当发生移动事件的时候，原有高级节点上的数据需要在变化之后的~$n'$~个节点中的~$s'$~个高级节点中重新分布，
\begin{eqnarray}
	n' = & \!\!\!\!n+\Delta{}n, & \Delta{}n > -n\\
	s' = & \!\!\!\!s+\Delta{}s, & \Delta{}s > -s\\
	      &	    		& 0\le{}\frac{\Delta{}s}{\Delta{}n}\le{}1.\nonumber
	\label{equ:nodesch}
\end{eqnarray}

移动个人网络中所涉及到的系统信息分为两类，其一是设备提供的能力服务器的描述信息，另一类则是系统中的应用会话相关信息。如表~\ref{tab:server}，一个能力服务器的描述信息不仅包含它的标识~(ServerID)、其提供的服务类型与能力等信息，同时还包括一个组标识~(GroupID)~用于描述其所在的组，这些信息可以用于帮助在服务发现的过程中应用选择恰当的服务器。而应用的描述信息~(表~\ref{tab:app})~则用于保存应用状态、维持业务连续性，其中包含了应用的~ID~和正在使用的或需要的能力服务器的相关描述。这样，如果系统现有能力无法维持业务继续运行则会终止应用，如果系统中有了新的能力服务器可用可以通知应用，甚至在能力服务器发生更替的时候可以及时利用先有资源重新生成业务、维持业务的连续性，这里，用于回调的地址可以是本地移动个人网络中的，也可以包含广域网络中的远程服务器地址描述。

\begin{table}
	\centering
	\caption{能力服务器的描述信息}
	\label{tab:server}
	\begin{tabular}{lll}
		\hline
		数据字段		& 描述内容							& 数据类型	\\
		\hline
		ServiceTypeID	& 服务器提供的服务类型				& string	\\
		ServerID		& 能力服务器的惟一标识				& string	\\
		DeviceID		& 提供能力服务器的设备			 	& string 	\\
		GroupID 		& 能力服务器所在设备的所属组		& string 	\\
		NormalizedCapa 	& 能力服务器的归一化能力描述		& integer 	\\
		UsedBy			& 目前使用此能力的应用				& string	\\
		TTL				& 此能力服务器描述的有效期			& integer	\\
		Capability		& 服务器能力的具体描述				& string	\\
		\hline
	\end{tabular}
\end{table}

\begin{table}
	\centering
	\caption{应用会话的描述信息}
	\label{tab:app}
	\begin{tabular}{lll}
		\hline
		数据字段		& 描述内容							& 数据类型	\\
		\hline
		AppID			& 应用会话的惟一标识				& string \\
		Location		& 应用的回调接口地址				& string	\\
		Required		& 应用所必须的能力服务器及能力		& string	\\
		Recommended		& 应用所需的其他能力服务器及能力	& string	\\
		Using			& 当前使用的能力服务器				& string	\\
		\hline
	\end{tabular}
\end{table}

所有这些记录都是由一组给定的键值对~(AVP, Attribute Value Pair)~组成的记录信息，并且具有各自的惟一标识，通过将这些数据放入统一的目录之中，即可通过各种组合查询手段找到需要的能力服务器或应用会话相关信息。而上述两类信息作为服务发现机制的数据源和结果又具有一些不同属性，例如，能力服务器信息是依附于其所在设备的，而设备是具有成组移动特性；相反，应用是全局性的信息，不带有分组性，并且，在发生移动的时候，业务应尽量保持连续性。

%In proposed protocol, only application entries are maintained system-wise so that the applications can be recomposed even if serving devices leave the system, while the CS entries of devices are maintained in the scope of group, therefore, the minimization of system-wise status \cite{bib:prehofer05} cuts down the cost of reorganizing the directory. All the SNs in mobile personal networks have some memory for maintaining a database of the directory entries for service discovery.
%后面分析中可以考虑把分布范围放在组或全局，考量不同的性能影响，不过下面那个一定是考虑组移动性的否则似乎优点不大


\subsection{移动个人网络数据的无中心组织方法}

对于全局信息的存储，为了均衡各个节点的负担，提高系统的可靠性和可伸缩性，论文中采用了一种将数据以无中心方式分布到各个节点之上的方法，并通过一定的冗余度，保障在节点大规模损失的情况下的数据可靠性。数据信息会被组播到所有的高级节点，而每个高级节点都通过一个自己的优先级算法，将优先级较高的一些记录保存在自己的存储空间之中，而不保存优先级较低的记录。每条记录~(r)~在特定节点~(n)~的优先级~($P_n(r)$)~的计算是通过一个哈希函数~($h(\cdot)$)~来实现的：定义记录的~ID~和节点的~ID~哈希值的距离为该记录在此节点上的存储优先级
\begin{equation}
	P_{n}(r)=d(h(r)-h(n))
	\label{eq:priority}
\end{equation}
这里，哈希值的分布特性将决定存储的分布特征，为了保证数据分布的均匀性，论文选择了常用的~SHA-1~算法作为哈希函数，对于非特殊选择的~ID，其哈希值应遵循均匀分布。在这种情况下，对于存储于~$s$~个高级节点中的~$r$~条信息，如果每个高级节点存储~$\gamma$~条信息，则信息冗余度大致为
\begin{equation}
	\rho=\frac{\gamma{}s}{r}.
	\label{eq:redundancy}
\end{equation}
对于哈希函数一定的取值范围，上述距离实际是一个圆周距离的概念，如图~\ref{fig:hashoverview}，高级节点会优先保存哈希值具体自己的哈希值比较近的~$R$~条记录，当记录条目比较多的时候，一个节点的覆盖范围就比较狭窄，而当记录条目较少的时候，节点发覆盖范围就比较宽，而冗余度本身则取决于节点的覆盖范围和节点密度两者的比值。

\begin{figure}[htb]
	\begin{center}
		\includegraphics[width=.9\textwidth]{hash-overview}
	\end{center}
	\caption{高级节点所保存的记录的键值范围示意}
	\label{fig:hashoverview}
\end{figure}

\section{目录数据向新加入节点传播}

在发生服务发现事件的时候，高级节点会自然地更新其存储的数据，除此之外，它们还通过目录更新消息来更新彼此存储的目录信息，每个高级节点都会以一定时间周期来公布它所存储的目录数据，这个更新周期~$T_u$~是由其上一次收到目录更新消息的时间~$T_{u0}$~和其存储的目录数据所确定的
\begin{equation}
	T_u = T_{u0} + T_r\cdot{}(R-R_D+1),
	\label{eqn:tupdate}
\end{equation}
其中，$T_r$~是一个均匀分布的随机变量，用以避免碰撞，$R$~是本节点所存储的目录数据的总量，而~$R_D$~是目录中较老的，没有在近期内被更新过的记录的数量，也就是说，有较多未被更新的数据的节点更倾向于较早地发送更新消息。一旦有新节点加入到移动个人网络之中，它们可以通过的目录更新消息得初始化本地目录信息，随后，与正常的目录更新过程一样，它们可以利用得到的目录更新消息调整目录内容，在若干个更新周期之后达到稳定状态。图~\ref{fig:spread-stat}~是单个高级节点的目录数据更新的状态图，图~\ref{fig:replica}~是这一过程的伪代码。

\begin{figure}[tb]
	\begin{center}
		\includegraphics[width=.7\textwidth]{spread-st}
	\end{center}
	\caption{高级节点目录更新状态转移图}
	\label{fig:spread-stat}
\end{figure}

\begin{figure}[htb]
	\begin{center}
	\framebox{%
	\begin{minipage}[t]{.9\textwidth}%
	\texttt{%
\mbox{~}on\_timeout()\\
\mbox{~}\hspace{4ex} sent\_update();\\
\mbox{~}\\
\mbox{~}on\_receive\_solicitation()\\
\mbox{~}\hspace{4ex} t = timer.get\_time();\\
\mbox{~}\hspace{4ex} timer.set\_time( t/entry\_number );\\
\mbox{~}\\
\mbox{~}on\_receive\_update()\\
\mbox{~}\hspace{4ex} time\_stamp = systemtime();\\
\mbox{~}\hspace{4ex} update\_entries(received\_data,time\_stamp);\\
\mbox{~}\hspace{4ex} entry\_diff\_count=0;\\
\mbox{~}\hspace{4ex} for i=1 to record\_number\\
\mbox{~}\hspace{4ex}\hspace{4ex}  if entry[i].time\_stamp < (time\_stamp-threshold)\\
\mbox{~}\hspace{4ex}\hspace{4ex}\hspace{4ex}   entry\_diff\_count++;\\
\mbox{~}\hspace{4ex} timer.set\_time(rand()*\\
\mbox{~}\hspace{4ex}\hspace{4ex}\hspace{4ex}\hspace{4ex}\hspace{4ex} (entry\_number-entry\_diff\_number+1));%  
	}%
	\end{minipage}%
	}
	\end{center}
	\caption{目录更新过程的伪代码}
	\label{fig:replica}
\end{figure}

由于目录更新消息是在高级节点之间以组播方式传送的，因此，当同时加入节点的数量增加的时候，目录达到稳定的时间不会线性增长，达到稳定的时间至多不会比所有的高级节点将全部目录数据发送出来。

假设当前的网络中有~$s$~个已经处于稳定状态的高级节点，这些节点中所有记录都处于未过期状态，即~$R_D=0$，这时，每个节点的定时器溢出时间都服从一个区间~$[T_{u0},T_{u0}+T_{ur}]$~上的均匀分布，其中~$T_{ur}=T_r\cdot (R+1)$，而对于系统整体来说，发送的时间将是所有节点发送定时器溢出时间的最小值
\begin{equation}
	t_{sent}=\min_{i=1}^{s}\left(T_{u0}+t_i\right)
	\label{equ:tomin}
\end{equation}
其中，$t_i$~是区间~$[0,T_{ur}]$~上服从均匀分布的随机变量，$T_{u0}+t_i$~是节点~$i$~的定时器溢出时间。这样，目录更新消息发送时间早于~$T_{u0}+x$~的概率为
\begin{eqnarray}
	F\left(t_{sent}<T_{u0}+x\right) & = & F\left(\min_{i=1}^s\left(t_i\right)<x\right)\nonumber\\
	                                & = & 1 - F\left(t_1>x,t_2>x,\cdots,t_s>x\right)
	\label{eq:propsent1}
\end{eqnarray}
不妨设各个节点的计时器溢出时间的概率分布是独立的，这样
\begin{equation}
	F\left(t_{sent}<T_{u0}+x\right)  =  1 - \prod_{i=1}^{s}F\left(t_i>x\right)
	\label{eq:propsent2}
\end{equation}
由于各个节点的计时器溢出时间都遵循均匀分布随机，所以
\begin{equation}
	F\left(t_{sent}<T_{u0}+x\right) = \begin{cases}
		1 - \frac{\left(T_{ur}-x\right)^s}{\left(T_{ur}\right)^s} & \text{if} \quad 0 \le x \le T_{ur} \\
		0                                            & \text{otherwise}
	\end{cases}
	\label{eq:propsent3}
\end{equation}
这样，目录更新消息发送，即最短计时器溢出时间的概率密度函数$\frac{dF}{dx}$为
\begin{equation}
	f\left(x\right)=\begin{cases}
		\frac{s\left(T_{ur} - x\right)^{s-1}}{T_{ur}^s} & \text{if} \quad 0 \le x \le T_{ur}\\
		0 & \text{otherwise}
	\end{cases}
	\label{eq:propdense}
\end{equation}
图~\ref{fig:propdense}~绘制出了~$T_{ur}$~归一化的不同的~$s$~值对应的更新发送时间的概率密度函数，可以看到，网络中的高级节点越多，发送目录更新消息的时间也就越早，其期望值是
\begin{eqnarray}
	E(x) & = & \int_{-\infty}^{\infty}x\cdot f(x)\,dx \nonumber\\
	     & = & \int_{0}^{T_{ur}} \frac{s x \left(T_{ur} - x\right)^{s-1}}{T_{ur}^s} dx \nonumber\\
     	 & = & \frac{ T_{ur}}{ \left( s + 1 \right)}
	\label{eq:expectsent}
\end{eqnarray}

\begin{figure}[tb]
	\begin{center}
		\includegraphics[width=.6\textwidth]{propdense}
	\end{center}
	\caption{目录更新消息发送时间的概率密度函数}
	\label{fig:propdense}
\end{figure}

当所有节点的数据记录都处在未过期状态时，各个节点以均等概率发送目录更新消息，如图~\ref{fig:hashoverlap}~所示的圆周上，若每个节点的覆盖范围的夹角为~$\theta=\frac{2\pi R}{R_{all}}$，其中~$R_{all}$~为总记录数，R~为每个节点存储的记录条数，两个节点之间的夹角为~$\phi$，则当~$\phi\ge\theta$~时两者不存在交叠区域，发送信息的总量即为~$2R$，否则，两次发送信息的总量存在一个交叠区域，两者覆盖面积总和为~$\theta+\phi$，发送的记录条数为~$\frac{R_{all}\left(\theta+\phi\right)}{2\pi}$。由于各个节点以均等概率发送目录更新消息，$\phi$~遵循~$[0,2\pi]$~上的均匀分布，所以，两次发送信息总量的数学期望~$E(R_s)$~为
\begin{eqnarray}
	E(R_s)  & = & \int_{0}^{2\pi}R_s(\phi)f(\phi)d\phi \nonumber\\
			& = & 2\int_{0}^{\theta}\frac{R_{all}(\theta+\phi)}{2\pi}\frac{1}{2\pi}d\phi%
			         + 2\int_{\theta}^{\pi}2R\frac{1}{2\pi}d\phi \nonumber\\
					 & = & (2-\frac{R}{R_{all}})\cdot R
	\label{eq:allsent}
\end{eqnarray}

\begin{figure}[tb]
	\begin{center}
		\includegraphics[width=.7\textwidth]{hash-overlap}
	\end{center}
	\caption{高级节点存储覆盖范围之间的交叠情况}
	\label{fig:hashoverlap}
\end{figure}

现在考虑两个节点所包含的较老数据记录数量不同的情况，假设两节点的~$R-R_D+1$~值分别为~$r_0$~和~$r_0+r$，其中~$r_0 \ge 1,r > 0, r_0+r \le R$，则两个节点的发送时间~$t_1$~和~$t_2$~将分别服从区间~$[T_{u0},T_{u0}+T_r\cdot r_0]$~和区间~$[T_{u0},T_{u0}+T_r\cdot\left(r_0+r\right)]$，这样，$t_1<t_2$~的概率则为
\begin{eqnarray}
	F(t_1<t_2) & = & \int_{-\infty}^{\infty}\int_{-\infty}^{t_2}f(t_1)f(t_2) dt_1 dt_2 \nonumber\\
	           & = & \int_{T_{u0}}^{T_{u0}+T_r\cdot r_0}\!\int_{T_{u0}}^{t_2}\frac{1}{T_r\cdot r_0}\cdot\frac{1}{T_r\cdot\left(r_0+r\right)}dt_1 dt_2 \nonumber\\
			   &   &  + \int_{T_{u0}+T_r\cdot r_0}^{T_{u0}+T_r\cdot\left(r_0+r\right)}\frac{1}{T_r\cdot\left(r_0+r\right)}dt_2\nonumber\\
			   & = & \frac{r_0+2r}{2(r0+r)}\nonumber\\
			   & = & 1-\frac{r_0}{2(r0+r)}
	\label{eq:proplt1}
\end{eqnarray}
也就是说，随着~$r$~的增加，具有较多较老数据记录的节点具有更高的较早发送目录更新消息的概率，当~$r>\!\!>r_0$~的时候，这一概率也趋近于~1。

由上述分析可知，系统中的全部数据以如下方式进行着~``循环''~流动，使得新加入节点可以读取数据并达到稳定：
\begin{itemize}
	\item 当所有节点保存的数据记录尚未过期的时候，各个保存有数据的节点会以略大于~$T_{u0}$~的时间间隔发送更新消息；
	\item 更新消息之间大致具有~$R/R_{all}$~的重复性，随着时间的推移，逐渐趋近于发送所有的数据记录；
	\item 一条数据记录如果在大约~$T_{threashold}/T_{u0}$~个更新周期内没有被任何节点发送过，将被标记为过期，具有较多过期记录的节点具有较高的率先发送更新消息的概率，从而加速了全部数据循环的速度。
\end{itemize}

表~\ref{tab:exam}~是一个节点加入过程的例子：假设原有~15~个高级节点，每个节点存储~8~条目录记录，而总共有~30~条记录，这样，每条记录大约有~4~个备份。表中情况为~4~个高级节点同时加入，它们通过目录更新消息来进行初始化。表~\ref{tab:exam}~中，每个表格代表了一个更新周期，表格的第一行则表示更新消息的内容，圆括号里的数字是发送目录更新消息的节点的编号；表中的其它每行都是代表一个新加入节点的目录内容，其中的内容是按照每条记录在该节点内的优先级来排序的，每次更新后新增加的记录用下划线标明。

如表~\ref{tab:exam}~所示，第一个更新周期之后，所有的新加入高级节点都使用来自~$s_6$~的更新消息进行了初始化。之后，使用来自~$s_5$~的更新消息，第一个新加入节点的数据几乎被完全更新，实际已经达到了稳定状态，而第二个新加入节点也向着稳定状态迈进了一步，其它两个节点则没有更新。最后，在第三个更新周期之中，所有新加入节点都同时进入了稳定状态。

\begin{table}[htb]
	\centering
	\caption{例：4~个高级节点加入一个原有~15~个高级节点的网络}
	\label{tab:exam}
	\begin{tabular}{p{10ex}|rrrrrrrr}
		\hline
		Updates(6) & 	12 &   9 &   1 &  21 &  19 &  17 &  24 &  6 \\
		\hline
		1			&	6  & 24  & 17  & 21  & 19  &  9  &  1  & 12 \\
		2			&	6  & 21  &  9  & 12  &  1  & 19  & 17  & 24 \\
		3			&   21 &   9 &  12 &   1 &   6 &  19 &  17 &  24 \\
		4			&	9  & 12  &  1  & 21  & 19  & 17  &  6  & 24 \\
		\hline
	\end{tabular}
	\vspace{1em}

	\begin{tabular}{p{10ex}|rrrrrrrr}
		\hline
		Updates(5) & 	13 &  27 &  15 &  14 &   7 &   5 &  11 &  16 \\
		\hline
		1			&   \underline{13} &  \underline{27} &  \underline{15} &  \underline{14} &   \underline{7} &   \underline{5} &  \underline{11} &  \underline{16} \\
		2			&    6 &  \underline{16} &  \underline{11} &   \underline{5} &  21 &   \underline{7} &   9 &  12 \\
		3			&   21 &   9 &  12 &   1 &   6 &  19 &  17 &  24 \\
		4			&    9 &  12 &   1 &  21 &  19 &  17 &   6 &  24 \\
		\hline
	\end{tabular}
	\vspace{1em}
	
	\begin{tabular}{p{10ex}|rrrrrrrr}
		\hline
		Updates(13) &   10 &  30 &   3 &  22 &  28 &   8 &   6 &   2 \\
		\hline
		1			&   13 &  27 &  15 &  14 &   7 &   5 &  11 &  16 \\
		2			&   \underline{28} &  \underline{22} &   \underline{3} &  \underline{30} &  \underline{10} &   \underline{2} &   \underline{8} &   6 \\
		3			&   21 &   9 &  12 &   1 &   6 &   \underline{8} &  \underline{10} &  19 \\
		4			&    9 &  12 &   1 &  21 &  19 &  17 &   6 &   \underline{8} \\
		\hline
	\end{tabular}
\end{table}

可以看到，所有节点的目录条目更新是并行进行的。诸如~chord~等~DHT~类的分布式存储算法，并不考虑这种大量节点同时加入的情况，而论文所述的方法因为更新消息是组播到所有高级节点的，所以，不仅可以平稳地处理这些节点的同时加入，而且可以以较高的效率并行处理，甚至在新加入高级节点的数量超过原有高级节点数量的时候也可以稳定工作。图~\ref{fig:spread}~所示的仿真工作就表明了这一点。

\begin{figure}[htbp]
	\begin{center}
		\subfloat[Chord~算法达到稳定所需消息数量]{\label{fig:chord-spread}%
			\includegraphics[width=.7\textwidth]{images/chord-spread}}\\
		\subfloat[论文所述方法达到稳定所需消息数量]{\label{fig:mpn-spread}%
			\includegraphics[width=.7\textwidth]{images/spread}}
	\end{center}
	\caption{节点成批加入时不同方法的达到稳定所需消息数量的对比}
	\label{fig:spread}
\end{figure}

%1 为啥是 chord
所有结构化~P2P~类方法都提供大致~$O(\log{}N)$~的查找复杂度，只是由于路由策略、键值空间和数据组织方式上的区别而在具体应用上性能有所差异\ucite{bib:lua2005}，但总体性能都在伯仲之间。因此，这里的仿真中，只选择了经典的~Chord~\ucite{bib:stoica2001,bib:stoica2003}~算法作为~DHT~类结构化~P2P~方法的代表，进行分析比较。仿真中，主要参考了文献~\cite{bib:stoica2003}~中的描述，使用~C~程序实现了~Chord~的数据稳定化算法~(\texttt{stablize()})、路由表修复算法~(\texttt{fix\_fingers()}~以及为了抗拒节点失效的复制算法~(\texttt{replica()})，并且随着~\texttt{stablize()}~算法调整存储在~Chord~环中的数据的位置。在节点加入的过程中，当数据不再迁移的时候，认为数据已经达到稳定，而当路由表也不再改变的时候，认为整个系统组织结构已经达到稳定，使用~Linux pthread~让上述机制分别独立、周期性地运行。从仿真数据~(图~\ref{fig:chord-spread})~可以看出，数据比整个系统组织略早达到稳定，但遵循相同的变化趋势，且比较接近。

%2 这么大差别的来源，chord 为啥有这么多操作
在仿真中，假设原有~15~个高级节点，每个节点存储总共~30~条目录条目中的~8~条，当加入的高级节点数目分别为~$1, 2, 4, 8,\cdots,128$~个的时候，比较论文所述算法和~Chord~算法在趋于稳定的过程中产生的消息量。我们发现，论文所述算法与~Chord~算法相比，达到稳定所需要的通信量相差很远~(图~\ref{fig:spread}，甚至无法在一张图上进行比较，这主要是因为论文所述算法是为个人移动网络所特别优化设计的，而~Chord~作为通用~P2P Overlay~由于最早是为广域网应用设计的，应用于移动个人网络时反而受到过多的负担的影响，消耗了更多的带宽资源。表~\ref{tab:p2pcomp}~对两种算法进行了比较。

\begin{table}[tb]
	\centering
	\caption{论文所述算法和~DHT~算法的比较}
	\label{tab:p2pcomp}
	\begin{tabular}{p{8em}p{0.3\textwidth}p{0.3\textwidth}}
		\hline
		比较项目		& DHT~类算法	& 论文所述算法	\\
		\hline
		信令传输方式	& 单播，节点间互相传递直至目的节点 & 组播，在所有节点中传输	\\
		影响数据存储位置的因素 & 节点间键值的关系、数据的键值，无法由节点自己判决，需要相互通信 & 节点的键值、节点内数据的键值关系，可以由节点自行决定\\
		插入节点前的准备& 需要通过查询确定需要插入节点的位置 & 无需准备，直接参与到数据更新过程之中 \\
		插入节点后的工作& 重新修订路由信息、转移存储的数据	& 数据更新，复制存储的数据 \\
		系统组织更新算法& 节点次序稳定算法和路由表修复算法两个并行算法，均会影响查询过程，进而相互影响，震荡趋于稳定 & 单一的目录更新算法，单调趋于稳定 \\
		\hline
	\end{tabular}
\end{table}

在广域网中，使用网络层组播的方式传递消息需要有相当的组播组维护开销和不小的组播消息开销，而正如~\S~\ref{sse:03:mpnchar}~中的分析，个人移动网络中，大量的节点分布在几跳之内的几个网段之中，而不是广泛地分布于互联网上，因而，使用组播并不会带来额外的开销和流量。同时，由于论文所述目录应用中不需要存储大型文件数据，因此，直接将数据发送出来，进行目录更新不会占用过高的带宽。

%3 不同的趋势
而且，如图~\ref{fig:spread}~所示，当新加入节点数量增长的时候，两种算法体现了不同的变化趋势，一方面~Chord~在加入节点数超出系统中原有的节点数的情况下，需要的通信流量的增长速度明显超过了线性增长，而论文所述算法则趋于平缓，增长幅度元小于加入节点数量的增长，这是因为，对于~Chord~算法，不考虑节点同时加入的情形，每个新加入的节点都会对已有系统组织结构造成一定影响，从而增加了查找难度，使得加入位置的准确率下降，而系统节点次序稳定算法和路由修复算法也会受到新加入节点的影响，新加入节点越多，造成系统不稳定的因素就越多，性能随之恶化，因此，此算法不适用于移动个人网络之中。而与之相反，论文所述方法可以并行地对多个节点进行初始化，并且在系统目录更新算法执行过程中，每个节点都在逐渐趋近于自己的稳定状态，当全部目录信息被发送出来之后，所有节点一定可以达到稳定状态。

图~\ref{fig:mpn-spread}~中，随着加入节点数量的指数增长，新加入节点所需要的目录更新周期数量以一条想对平缓的曲线增长，这一增长是由于新加入的未稳定节点同样参与了竞争发送目录更新消息的行列，从而延缓了整个系统中达到稳定状态的时间，使得系统稳定的时间不能达到一个常数，这个增长的过程受到更新消息发送计时器的设置的影响，图~\ref{fig:mpn-spread}~显示，较高的更新计时器定时值设置可以帮助新加入节点尽早达到稳定状态，因为这样可以让较老的数据更快地被标记为过期，从而被增大发送概率。

\section{节点退出时数据的存活能力}

论文所述方法中，每个节点所保存的记录条目是一定的，这样，当节点~ID~的哈希值和存储数据的哈希值分布均匀的时候，可以趋近于式~\ref{eq:redundancy}~所述的理想冗余度，而在~Chord~等完全结构化~P2P~系统中，通过确保每个数据存储在~$r$~个节点之上，在哈希值分布均匀的情况下，每个节点所保存的记录条目数是趋于稳定值的。

设系统中共有~$R_{all}$~条记录，每个节点保存其中~$R$~条，占其中的~$p=\frac{R}{R_{all}}$，考虑系统中的节点和记录的键值都服从均匀分布，则，任取某条数据，其包含在某个节点中的概率为~$p$，这样，对于包含~$s$~个高级节点的系统，一条记录存在于~$x$~个节点中的事件服从参数为~$s,p$~二项分布，而该记录其被包含在不少于~$t$~个节点之中的概率为
\begin{eqnarray}
	P[x>t]  & = & \sum_{k=t}^{s} {{s \choose k} p^k (1-p)^{s-k} } \nonumber \\
			& = & 1 - \sum_{k=0}^{t-1} {{s \choose k} p^k (1-p)^{s-k}}
	\label{eq:replica}
\end{eqnarray}
其中，$s\cdot{}p$~即是冗余度的期望值，这个值的大小几乎左右了最终的概率。图~\ref{fig:replica-prop}~更形象地示意了节点数量和每个节点的记录条目占整体记录条目数的比例与记录冗余度的关系，当节点数量增加或节点的记录条目占所有记录条目的比例增加的时候，任一记录达到某个给定值的概率会趋近于~1，从图中~s-p~平面上绘制出的等高线可以看出，当节点数量~($s$)~与节点记录条目占总条目数的比例~($p$)~之积接近~6~的时候，任取一记录的冗余度均大于~4~的概率即超过~$90\%$，如下仿真数据也验证了这一结论。

\begin{figure}[htb]
	\begin{center}
		\includegraphics[width=.8\textwidth]{replica-prop}
	\end{center}
	\caption{节点数量、记录条目数与冗余度的关系}
	\label{fig:replica-prop}
\end{figure}

这里，我们依然使用~Chord~算法~\ucite{bib:stoica2001}~进行对比，使用参考文献~\cite{bib:stoica2003}~中描述的数据冗余方法，将同一数据同时保存在键值对应的节点和其后的几个节点之上，这样可以确保冗余度为常数。在前文仿真所使用的系统中，初始情况下，30条记录保存在~15~个节点中，冗余度为~4，此时，不同节点上的数据量分布如图~\ref{fig:replica-chord}~所示，每个节点存储着~2-15~条不等的数据记录，这样，作为系统中的每个节点，需要参照极大值，为这些记录留出相应的存储空间。

\begin{figure}[htb]
	\begin{center}
		\includegraphics[width=.7\textwidth]{replica-chord}
	\end{center}
	\caption{Chord~算法的存储空间消耗}
	\label{fig:replica-chord}
\end{figure}

而本文所述算法与~Chord~不同，每个节点固定保存一定数量的记录数量，同样以上述~15~个节点和~30~条记录条目为例，每条数据条目的冗余度如图~\ref{fig:replica-mpn}~所示，当每个节点存储~8~条数据时，这时，数据冗余度的期望值为~4，和~Chord~算法相当，这时，数据的实际冗余度介于~2-6~之间，而当令每个节点存储~12~条数据~(仍小于~Chord~的节点存储空间的最大值)~时，冗余度的期望值为~6，且所有数据的冗余度均大于~4。可见，论文所述算法在节点存储开销相当的情况下，与~Chord~可以达到同等的冗余度。

\begin{figure}[htb]
	\begin{center}
		\includegraphics[width=\textwidth]{replica-mpn}
	\end{center}
	\caption{论文所述算法的记录冗余度}
	\label{fig:replica-mpn}
\end{figure}

%\section{查找数据的性能}

%确知的知道多少步骤可以找到所需数据

\section{小结}

本章提供了一种~``半结构化''~的无中心数据存储机制，用于分布式地存储系统中的可用资源信息与系统中的业务应用的状态信息，保障这些数据在用户移动过程中的可靠性，在系统中节点发生大规模变化的时候仍然具有可用性，并为移动个人网络中所特有的大量节点成组加入或退出进行优化设计，保障移动性在事件发生时系统保持稳定。与已有的结构化无中心存储算法相比方法相比，论文设计的方法利用了移动个人网络的特性，占用较少的通信带宽、在同等节点存储资源消耗下，提供相似的数据可靠性，并且更能适应同时大量节点变更的情况。

